---
title: "Proposal title"
subtitle: "Proposal"
author: 
  - name: "Joshua Dunlap"
    affiliations:
      - name: "Finding and Measuring the Temporality Dimension in Historical Static Word Embedding Models"
description: "This project will create a dimension of temporality---basically how future or past-oriented a concept is---in a word embedding model."
format:
  html:
    code-tools: true
    code-overflow: wrap
    code-line-numbers: true
    embed-resources: true
editor: visual
code-annotations: hover
execute:
  warning: false
jupyter: python3
---

```{python}
#| label: load-pkgs
#| message: false
import numpy as np
```

## Dataset

```{python}
#| label: load-dataset
#| message: false
import gensim.downloader as api
from numpy import dot
from numpy.linalg import norm
import numpy as np

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import pandas as pd
model = api.load("glove-wiki-gigaword-100")  # 100d GloVe vectors


```

I currently have access to glove-wiki-gigaword-100. If I am unable to use a dataset more directly relevant to my research, I will use glove-wiki-gigaword-100. It is a relevant dataset because it has been used as a standard static word embedding model for all sorts of NLP tasks, and it is trained on enough text that if the temporality dimension is shown to work in the context of glove-wiki-gigaword-100 then show that at least it will work the gold standard NLP environment.

My hope, however, is to access a dataset more directly suitable to my research. My ideal would be over 100 years of historical newspaper data and I’m currently pursuing several options to access a dataset like this. If I gain access I will also need to train static word embedding models on the data, which will add some amount of technical wrangling on the front end. 

Update 11/13

I am working with a research librarian at the university to try to use this dataset: ProQuest Historical Newspapers Collections: The New York Times (1851-1936), which is much closer to the dataset that I intend to eventually use for this research project. I'm not sure if I will have access in time to complete the project with this data, but progress is being made on this front!

One important note about whichever dataset is used is that it will necessarily embody the cultural position and biases of the corpus. Groundbreaking studies have demonstrated, for example, the gender bias present in many large text corpora. For the purposes of this study, this means that I will need to be aware that the temporality dimension that is found will be one that is reflective of the viewpoint of the corpus, not a universal depiction of temporality in the English language.

## Questions

Can I find a temporality dimension in the data? Is this dimension robust enough to solve analogy problems?

I am interested in these questions at a broad level because I am curious about the history of future visioning. In order to understand how people in the past thought about the future, it will be worthwhile to have decade-specific embeddings of temporality so that we can understand what was considered futuristic at different points in time. More specifically, I'm interested in finding a temporality dimension in large text corpora for a specific research project related to understanding people's shifting conceptions of "utopian" visioning---how this concept shifts back and forth between being past and future-oriented. My hypothesis is that this is related to broader cycles of expanding and contracting senses of what is politically possible and desirable.

## Analysis plan

Can I find a temporality dimension in the data?
I will answer this question by emulating past research which has shown that it is possible to identify "cultural dimensions" running through word embedding models using a technique where you subtract and average a bunch of paired antonyms related to a cultural concept. So, for example, they find the "affluence" dimension that runs through the model by subtracting the vector for "rich" from the vector for "poor", the vector for "wealthy" from the vector for "impoverished", etc. This then allows them to place other concepts in relation to this "affluence" dimension, finding, for example, that "tennis" and "opera" are on the wealthy side of the affluence plane.

I am going to do this in relation to temporality; subtracting "future" from "past", "modern" from "vintage," etc. The new variable to be created will be this vector, which concepts can be measured against (is “spaceship” on the future side? is “archive” on the past side?) External data that might be necessary is incorporating information from current and historical thesauri in order to create a sufficiently robust set of paired antonyms. 

Is this dimension robust enough to solve analogy problems?
One of the most famous examples of the semantic relationships embedded in these static models is the vector for “king” minus the vector for “man” plus the vector for “woman” equals “queen.” The underlying principle is that semantic relationships are so strongly embedded that analogies can be “mathematically solved.” 

Can these analogy problems be extended? If we invert the position of "steamboat" on the temporality dimension would we find nearest neighbor words like "submarine." What about if we take the inverse temporality position of "e-reader"? Might we find something like "papyrus"?

```{python}
#| label: load-dataset
#| message: false
result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)

for word, similarity in result:
    print(f"{word}: {similarity:.3f}")


```

Can this be extended to analogies that rely on temporality? Can I show that the temporality dimension aligns in predictable ways with semantic expectations?

